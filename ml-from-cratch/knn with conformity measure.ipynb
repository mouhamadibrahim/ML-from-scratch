{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the goal of this project is to learn and implement k-nearest neighbor algorithm from scratch. I will be explaining everything in this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we are goin to calculate knn and average p-false for iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO \n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step after loading data is to split them into training and testing data.Basically, the scikit-learn function train_test_split splits the data into training (75%) and testing (25%). its a good rule of thumb to divide the data like this but we can always change it. the random_state was set to my DDMM for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'],\n",
    "iris['target'], random_state=408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the euclidean distance is calculated when we take two arrays of data, subtracting their features ( coordinates ) then taking the square root of the value\n",
    "\n",
    " ex:\n",
    "- training sample: (1,2)\n",
    "- test sample:(7,2)\n",
    "\n",
    "the euclidean distance is: sqrt((1-7)** 2 + (2-2)** 2)\n",
    "\n",
    "so in here we took eu_distance we set it to 0 and everytime we calculated a new coordinates we add it to the distance( alternativaly we could've used np.sum to do so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1,x2,number_of_features):\n",
    "    eu_distance = 0\n",
    "    for index in range(number_of_features):\n",
    "        eu_distance += (x1[index] - x2[index])**2\n",
    "    return np.sqrt(eu_distance) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Predict Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the predict method basically takes in training set,test set and the number of features and just calls the euclidean distance used before to produce all the distances from the test sample to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train_set,test_set,number_of_features):\n",
    "    distance = []\n",
    "    for index in range(len(train_set)):\n",
    "        distance.append([euclidean_distance(train_set[index],test_set,number_of_features),train_set[index][number_of_features]])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after getting all the distances from the test sample to the training set we need to get the minimum distance to that set so in order to do so we need to sort the distances first this will return them in a ascendant order ( from minimum to maximum ) then we need to take the label of the first one which is our calculated label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1724,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(predictions,k):\n",
    "    predictions.sort()\n",
    "    subset_of_predictions = predictions[:k]\n",
    "    labels = []\n",
    "    for index in subset_of_predictions:\n",
    "        labels.append(int(index[1]))    \n",
    "    frequent_value = Counter(labels).most_common(1)\n",
    "    return frequent_value[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Error Rate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the test error rate basically takes in the calculated labels from the previous function and compares the values then returns the mean of them.\n",
    "\n",
    "ex:\n",
    "\n",
    "if a calculated label was 1 but the y_test label was 0 the output would be False so we want to get the mean of how many values we missed predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_rate(predictions, actual_labels):\n",
    "    return np.mean(predictions != actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate labelled samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, i want to start by adding the label to the features in this way i would have labelled features, so in order to do i want to loop over the length of the features that i have, then concatinating the features and the labels in a numpy array. so first i have defined the numpy array using the shape of the length of the features and the features shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(features,labels):\n",
    "    labelled = np.zeros((len(features),features.shape[1]+1))\n",
    "    for index in range(len(features)):\n",
    "        labelled[index] = np.concatenate([features[index],[labels[index]]])\n",
    "    return labelled    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformity Score for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we need to create a conformity score for training data before encountering the test data. basically the conformity score is when we take one sample computer the distance to all other samples in the training set, then we get the nearest distance to the same data label and the nearest to the different label, divide them and that is how we get the conformity score.\n",
    "\n",
    "so what we did here is that we took the training set,the label of the training set and a pandas dataframe ( the reason i used pandas here is that visualizing the conformity score in a column is much more easier and better than seeing it inside an array plus it will help debbug even better if one answer is not very correct ) also this column is being used later on.\n",
    "\n",
    "so in here we calculate the distance of each training sample to all other samples of the same label and of different label, sorting them out from minimum to maximum then getting the first one of each two list, dividing them and getting the conformity score of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have introduced some statements that will make the calculation faster when encountered some problems:\n",
    "- 0/0 = 1 \n",
    "- number/0 = inf\n",
    "- 0/x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1699,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformity_score_for_training_data(X_train_copy,y_train_copy,df):\n",
    "    for i in range(len(X_train_copy)):\n",
    "        same_class = []\n",
    "        different_class =[]\n",
    "        same_class_labelled = []\n",
    "        different_class_labelled = []\n",
    "        number_of_features = len(X_train_copy[0]) - 1\n",
    "        for j in range(0,len(X_train_copy)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if (X_train_copy[i][number_of_features] == X_train_copy[j][number_of_features]):\n",
    "                same_class.append(euclidean_distance(X_train_copy[i],X_train_copy[j],number_of_features))\n",
    "            else:\n",
    "                different_class.append(euclidean_distance(X_train_copy[i],X_train_copy[j],number_of_features))\n",
    "\n",
    "        same_class.sort()\n",
    "        same_class_labelled = same_class[0]\n",
    "\n",
    "        different_class.sort()\n",
    "        different_class_labelled = different_class[0]\n",
    "\n",
    "        if different_class_labelled == 0:\n",
    "            conformity_measure = 0 \n",
    "            df.at[i,\"conformity score\"]= conformity_measure\n",
    "        elif different_class_labelled and same_class_labelled == 0:\n",
    "            conformity_measure = 1 \n",
    "            df.at[i,\"conformity score\"]= conformity_measure\n",
    "        elif same_class_labelled == 0:\n",
    "            conformity_measure = Math.inf \n",
    "            df.at[i,\"conformity score\"]= conformity_measure\n",
    "        else:\n",
    "            conformity_measure = (different_class_labelled/same_class_labelled) \n",
    "            df.at[i,\"conformity score\"]= conformity_measure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conformity score for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in here we are doing the same thing as we did in the training set in the sense of calculating the conformity score where we take each test sample, calculate the conformity score, but after that we want to get the p-value where the p-value is that we need to get the rank.\n",
    "\n",
    "basically, when we get the conformity score we need to calculate the rank where i created a loop that will increase the counter everytime a higher number than the conformity score of the test sample found, the we can get the p-value by dividing the rank/len of training data + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformity_score_for_test_data(X_test,y_train):\n",
    "    #extract unique labels in order to calculate the p-value of the test sample for each label\n",
    "    unique_values = np.unique(y_train)\n",
    "\n",
    "    #make copy of the test data\n",
    "    X_test_copy = X_test\n",
    "\n",
    "    all_test_values = []\n",
    "    calculate_labels = []\n",
    "\n",
    "    test_labelled_p_value = []\n",
    "\n",
    "    #i will take values between 0 and length of test data\n",
    "    for i in range(len(X_test_copy)):\n",
    "        values = []\n",
    "        #unique will take values of labels\n",
    "        for unique in unique_values:\n",
    "\n",
    "            same_class = []\n",
    "            different_class =[]\n",
    "            same_class_labelled = []\n",
    "            different_class_labelled = []\n",
    "            max_value = []\n",
    "\n",
    "            #j will take values between 0 and the length of training data (each test sample will calculate the distance to all train data)\n",
    "            for j in range(0,len(X_train_copy)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                #checking if the label of training data matches the value of the first label taken\n",
    "                if (unique == X_train_copy[j][4]):\n",
    "                    #if yes add the value to the same_class where it will calculate the euclidean distance\n",
    "                    same_class.append(euclidean_distance(X_train_copy[j],X_test_copy[i],number_of_features))\n",
    "                else:\n",
    "                    #if not add the uclidean distance calculated to the different_class pile\n",
    "                    different_class.append(euclidean_distance(X_train_copy[j],X_test_copy[i],number_of_features))\n",
    "\n",
    "            #sort the distances of the same class to take the minimum one        \n",
    "            same_class.sort()\n",
    "            #take the minimum value\n",
    "            same_class_labelled = same_class[0]\n",
    "\n",
    "            #sort the distances of the different class to take the minimum one        \n",
    "            different_class.sort()\n",
    "            #take the minimum value\n",
    "            different_class_labelled = different_class[0]\n",
    "\n",
    "            #taking some measures here ( talk about them in a different cell)\n",
    "            if different_class_labelled == 0:\n",
    "                conformity_measure = 0 \n",
    "                values.append([conformity_measure,unique])\n",
    "            elif different_class_labelled and same_class_labelled == 0:\n",
    "                conformity_measure = 1 \n",
    "                values.append([conformity_measure,unique])\n",
    "            elif same_class_labelled == 0:\n",
    "                conformity_measure = Math.inf \n",
    "                values.append([conformity_measure,unique])\n",
    "            else:\n",
    "                #callculate the confomity score which is distance to the nearest of different class / same class\n",
    "                conformity_measure = (different_class_labelled/same_class_labelled) \n",
    "                #add the conformity measure to the values variable\n",
    "                values.append([conformity_measure,unique])        \n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average False P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, for the false p-value we need to calculate the p-value for each row and for each label( ex: if we have 3 labels we will get three p-value pa,pb,pc) then we need to check the label and remove the corresponding p-value and calculate the average of the remaining p-value. then, we need to sum all the values and divide them by the length of labels to get the final false average p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1721,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_false(test_labelled_p_value,y_test):\n",
    "    average_false_p_value = []\n",
    "    for i in range(len(test_labelled_p_value)):\n",
    "        value_to_delete = y_test[i]\n",
    "        del test_labelled_p_value[i][value_to_delete]\n",
    "\n",
    "    for i in test_labelled_p_value:\n",
    "        result = 0\n",
    "        for j in range((len(unique_values)-1)):\n",
    "            result += i[j]\n",
    "        average_false_p_value.append(result/2)\n",
    "    return sum(average_false_p_value)/38     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average false p-value entire code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_false_p_value(y_train,labelled_train_set,X_train,X_test,df):    \n",
    "    unique_values = np.unique(y_train)\n",
    "\n",
    "    X_train_copy = labelled_train_set\n",
    "\n",
    "    all_test_values = []\n",
    "    calculate_labels = []\n",
    "\n",
    "    test_labelled_p_value = []\n",
    "\n",
    "    number_of_features = len(X_train[0])\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        values = []\n",
    "        for unique in unique_values:\n",
    "            same_class = []\n",
    "            different_class =[]\n",
    "            same_class_labelled = []\n",
    "            different_class_labelled = []\n",
    "            max_value = []\n",
    "\n",
    "            for j in range(0,len(X_train_copy)):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                if (unique == X_train_copy[j][number_of_features]):\n",
    "                    same_class.append(euclidean_distance(X_train[j],X_test[i],number_of_features))\n",
    "                else:\n",
    "                    different_class.append(euclidean_distance(X_train[j],X_test[i],number_of_features))\n",
    "\n",
    "            same_class.sort()\n",
    "            same_class_labelled = same_class[0]\n",
    "\n",
    "            different_class.sort()\n",
    "            different_class_labelled = different_class[0]\n",
    "\n",
    "            if different_class_labelled == 0:\n",
    "                conformity_measure = 0 \n",
    "                values.append([conformity_measure,unique])\n",
    "            elif different_class_labelled and same_class_labelled == 0:\n",
    "                conformity_measure = 1 \n",
    "                values.append([conformity_measure,unique])\n",
    "            elif same_class_labelled == 0:\n",
    "                conformity_measure = Math.inf \n",
    "                values.append([conformity_measure,unique])\n",
    "            else:\n",
    "                conformity_measure = (different_class_labelled/same_class_labelled) \n",
    "                values.append([conformity_measure,unique]) \n",
    "        rank = []\n",
    "        p_value = []\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            number_of_training_set = len(df['conformity score'])\n",
    "            counter = 0\n",
    "            result = 0\n",
    "            value = 0\n",
    "            for j in range(number_of_training_set):\n",
    "                if values[i][0] <= df['conformity score'][j]:\n",
    "                    counter +=1\n",
    "            if counter == number_of_training_set:\n",
    "                result = 1\n",
    "                value = 1/ (number_of_training_set + 1)\n",
    "                rank.append(result)\n",
    "                p_value.append(value)\n",
    "            else:\n",
    "                result = number_of_training_set - counter\n",
    "                rank.append(result)\n",
    "                value = result / (number_of_training_set + 1)\n",
    "                p_value.append(value)\n",
    "        test_labelled_p_value.append(p_value)\n",
    "    average_false_p_value = []\n",
    "    for i in range(len(test_labelled_p_value)):\n",
    "        value_to_delete = y_test[i]\n",
    "        del test_labelled_p_value[i][value_to_delete]\n",
    "\n",
    "    for i in test_labelled_p_value:\n",
    "        result = 0\n",
    "        for j in range((len(unique_values)-1)):\n",
    "            result += i[j]\n",
    "        average_false_p_value.append(result/2)\n",
    "    return sum(average_false_p_value)/38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main for knn Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error rate for k=1 for iris dataset is: 0.07894736842105263\n",
      "average false p-value for iris is: 0.010829063809967404\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'],\n",
    "iris['target'], random_state=408)\n",
    "\n",
    "iris_train_set = fit(X_train, y_train)\n",
    "iris_test_set = fit(X_test, y_test)\n",
    "\n",
    "number_of_features = len(X_train[0])\n",
    "\n",
    "predictions = []\n",
    "predicted_labels = []\n",
    "k = 1\n",
    "\n",
    "for index in range(len(iris_test_set)):\n",
    "        predictions.append(predict(iris_train_set,iris_test_set[index],number_of_features))\n",
    "        predicted_labels.append(nearest_neighbor(predictions[0],k))        \n",
    "        predictions = [] \n",
    "\n",
    "print(\"test error rate for k=1 for iris dataset is:\",test_error_rate(predicted_labels,y_test))\n",
    "        \n",
    "df = pd.DataFrame(data=iris_train_set)\n",
    "conformity_score_for_training_data(iris_train_set,y_train,df)\n",
    "\n",
    "#make copy of the test data\n",
    "X_test_copy = X_test\n",
    "calculated_p_value = 0\n",
    "\n",
    "average_false = 0\n",
    "\n",
    "average_false = average_false_p_value(y_train,iris_train_set,X_train,X_test,df)\n",
    "\n",
    "print(\"average false p-value for iris is:\",average_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main for Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test error rate for k= 1  for ionosphere is: 0.056818181818181816\n",
      "average false p-value for iris is: 0.051580948569141534\n"
     ]
    }
   ],
   "source": [
    "ionosphere_data = np.genfromtxt(\"ionosphere.txt\", delimiter=',', names=True, dtype=None)\n",
    "\n",
    "data = [list(z) for z in ionosphere_data]\n",
    "ionosphere_X = []\n",
    "ionosphere_y = []\n",
    "for i,k in enumerate(data):\n",
    "    ionosphere_X.append([z for v,z in enumerate(k) if v!=len(k)-1])\n",
    "    ionosphere_y.append([z for v,z in enumerate(k) if v==len(k)-1])\n",
    "\n",
    "ionosphere_X = np.array(ionosphere_X)\n",
    "ionosphere_y = np.hstack(ionosphere_y)\n",
    "\n",
    "#since we have just 2 labels, i just changed them to 0,1 it looks better working with 0 and 1\n",
    "ionosphere_y = np.where(ionosphere_y == 1, 1, 0) #converting to 0 and 1 for simplicity\n",
    "\n",
    "#split the data and choose random_state as DDMM of my birthday\n",
    "X_train, X_test, y_train, y_test = train_test_split(ionosphere_X,\n",
    "ionosphere_y, random_state=408)\n",
    "\n",
    "#fit the data where we add the label with the features to get labelled samples\n",
    "ionosphere_train_set = fit(X_train, y_train)\n",
    "ionosphere_test_set = fit(X_test, y_test)\n",
    "\n",
    "number_of_features = len(X_train[0])\n",
    "\n",
    "predictions = []\n",
    "predicted_labels = []\n",
    "k = 1\n",
    "\n",
    "for index in range(len(ionosphere_test_set)):\n",
    "        predictions.append(predict(ionosphere_train_set,ionosphere_test_set[index],number_of_features))\n",
    "        predicted_labels.append(nearest_neighbor(predictions[0],k))        \n",
    "        predictions = []       \n",
    "\n",
    "print(\"the test error rate for k=\",k,\" for ionosphere is:\",test_error_rate(predicted_labels,y_test))\n",
    "        \n",
    "df = pd.DataFrame(data=ionosphere_train_set)\n",
    "conformity_score_for_training_data(ionosphere_train_set,y_train,df)\n",
    "\n",
    "X_test_copy = X_test\n",
    "calculated_p_value = 0\n",
    "\n",
    "average_false = 0\n",
    "\n",
    "average_false = average_false_p_value(y_train,ionosphere_train_set,X_train,X_test,df)\n",
    "\n",
    "print(\"average false p-value for iris is:\",average_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
